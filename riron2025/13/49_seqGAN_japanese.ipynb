{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30261617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\flare\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.20.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\flare\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (3.0.1)\n",
      "📖 Generator Pre-Training\n",
      "Epoch 0, Loss: 4.4581\n",
      "Epoch 50, Loss: 0.0141\n",
      "Epoch 100, Loss: 0.0042\n",
      "Epoch 150, Loss: 0.0021\n",
      "Epoch 200, Loss: 0.0012\n",
      "Epoch 250, Loss: 0.0008\n",
      "\n",
      "📖 Discriminator Pre-Training\n",
      "Epoch 0, Loss: 2.7101\n",
      "Epoch 20, Loss: 0.0808\n",
      "Epoch 40, Loss: 2.8082\n",
      "Epoch 60, Loss: 5.5304\n",
      "Epoch 80, Loss: 0.0226\n",
      "\n",
      "🎯 Adversarial Training (RL)\n",
      "Epoch 0, G Loss: 0.1018\n",
      "Epoch 10, G Loss: 0.0874\n",
      "Epoch 20, G Loss: 0.1585\n",
      "Epoch 30, G Loss: 0.1208\n",
      "Epoch 40, G Loss: 0.0016\n",
      "Epoch 50, G Loss: 0.0005\n",
      "Epoch 60, G Loss: 0.0010\n",
      "Epoch 70, G Loss: 0.0074\n",
      "Epoch 80, G Loss: 0.0005\n",
      "Epoch 90, G Loss: 0.0002\n",
      "\n",
      "🌸 生成サンプル\n",
      "生成文: く春や岩に岩に岩に岩\n",
      "生成文: 集岩に岩に岩に岩に岩\n",
      "生成文: 水さや岩に岩に岩に岩\n",
      "生成文: 寺に食食食食食食食食\n",
      "生成文: たさや岩に岩に岩に岩\n"
     ]
    }
   ],
   "source": [
    "# ✅ 超軽量SeqGAN（日本語版）完全版\n",
    "# Google Colab対応\n",
    "\n",
    "# ライブラリ\n",
    "!pip install torch torchvision tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "# 🌱 データセット（日本語短文）\n",
    "data = [\n",
    "    \"春の海ひねもすのたりのたりかな\",\n",
    "    \"古池や蛙飛びこむ水の音\",\n",
    "    \"柿食えば鐘が鳴るなり法隆寺\",\n",
    "    \"山路来て何やらゆかしすみれ草\",\n",
    "    \"夏草や兵どもが夢の跡\",\n",
    "    \"秋深き隣は何をする人ぞ\",\n",
    "    \"名月や池をめぐりて夜もすがら\",\n",
    "    \"閑さや岩にしみ入る蝉の声\",\n",
    "    \"五月雨を集めて早し最上川\",\n",
    "    \"行く春や鳥啼き魚の目は泪\"\n",
    "]\n",
    "\n",
    "chars = sorted(list(set(\"\".join(data))))\n",
    "char2idx = {c: i for i, c in enumerate(chars)}\n",
    "idx2char = {i: c for c, i in char2idx.items()}\n",
    "vocab_size = len(chars)\n",
    "\n",
    "def encode(text): return [char2idx[c] for c in text]\n",
    "def decode(indices): return \"\".join([idx2char[i] for i in indices])\n",
    "\n",
    "encoded_data = [torch.tensor(encode(sentence)) for sentence in data]\n",
    "\n",
    "# ⚙️ Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def sample(self, start_token, length):\n",
    "        result = [start_token]\n",
    "        input = torch.tensor([[start_token]])\n",
    "        hidden = None\n",
    "        for _ in range(length - 1):\n",
    "            logits, hidden = self.forward(input, hidden)\n",
    "            probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            result.append(next_token)\n",
    "            input = torch.tensor([[next_token]])\n",
    "        return result\n",
    "\n",
    "# ⚙️ Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embed_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch, 1, seq_len, emb_dim)\n",
    "        x = [torch.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [torch.max(t, dim=2)[0] for t in x]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        return self.sigmoid(self.fc(x))\n",
    "\n",
    "# 🌱 モデル初期化\n",
    "generator = Generator(vocab_size, 16, 32)\n",
    "discriminator = Discriminator(vocab_size, 16, 32, [2,3,4])\n",
    "\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# 🔥 事前学習：Generator\n",
    "print(\"📖 Generator Pre-Training\")\n",
    "for epoch in range(300):\n",
    "    for sentence in encoded_data:\n",
    "        inputs = sentence[:-1].unsqueeze(0)\n",
    "        targets = sentence[1:].unsqueeze(0)\n",
    "        logits, _ = generator(inputs)\n",
    "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
    "        g_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        g_optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 🔥 事前学習：Discriminator\n",
    "print(\"\\n📖 Discriminator Pre-Training\")\n",
    "real_label, fake_label = 1, 0\n",
    "for epoch in range(100):\n",
    "    for sentence in encoded_data:\n",
    "        real_data = sentence.unsqueeze(0)\n",
    "        fake_seq = generator.sample(random.choice(range(vocab_size)), len(sentence))\n",
    "        fake_data = torch.tensor([fake_seq])\n",
    "        \n",
    "        real_out = discriminator(real_data)\n",
    "        fake_out = discriminator(fake_data)\n",
    "        \n",
    "        real_loss = bce_loss(real_out, torch.ones_like(real_out)*real_label)\n",
    "        fake_loss = bce_loss(fake_out, torch.ones_like(fake_out)*fake_label)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        \n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {d_loss.item():.4f}\")\n",
    "\n",
    "# 🔥 強化学習：Adversarial Training\n",
    "print(\"\\n🎯 Adversarial Training (RL)\")\n",
    "for epoch in range(100):\n",
    "    for _ in range(len(data)):\n",
    "        start_token = random.choice(range(vocab_size))\n",
    "        fake_seq = generator.sample(start_token, 10)\n",
    "        fake_data = torch.tensor([fake_seq])\n",
    "        \n",
    "        reward = discriminator(fake_data).detach()\n",
    "        log_probs, _ = generator(fake_data[:,:-1])\n",
    "        log_probs = torch.log_softmax(log_probs, dim=-1)\n",
    "        selected_log_probs = log_probs.gather(2, fake_data[:,1:].unsqueeze(-1)).squeeze(-1)\n",
    "        loss = -torch.mean(selected_log_probs * reward)\n",
    "        \n",
    "        g_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        g_optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, G Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 🌸 サンプル生成\n",
    "print(\"\\n🌸 生成サンプル\")\n",
    "for _ in range(5):\n",
    "    start_token = random.choice(range(vocab_size))\n",
    "    generated = generator.sample(start_token, 10)\n",
    "    print(\"生成文:\", decode(generated))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
